{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5781e42e-6db3-496d-8746-13dc9b71c455",
   "metadata": {},
   "source": [
    "### Lunar Lander Environment\n",
    "Observation Space: 8-dimensional vector: the coordinates of the lander, its linear velocities, its angle, angular velocity, and bools representing whether each leg is touching the ground  \n",
    "[x, y, vx, vy, angle, angle_vel, left_leg_on_ground, right_leg_on_ground]  \n",
    "Action Space: 4 Discrete actions: do nothin, fire left orinetation engine, fire main engine, fire right orientation engine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582666d-bf8d-4c9f-9b0c-89e9546ae704",
   "metadata": {},
   "source": [
    "### Theory\n",
    "V(s) = max(Q(s,a)  \n",
    "Q(s,a) = R(s,a) + gV(s')  \n",
    "p(s) = max Q(s, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6593d1b2-b2b7-4c93-b5e5-72b45dc88dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0764c-ade3-4bf1-aa0d-49798c33f26c",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba427c7-fb73-4677-a47b-3a4a4bc7abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sys\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77a01fe-c282-4eb3-a793-9f7761ac84c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLAgent:\n",
    "    \"\"\"A learning Agent for the Lunar Landar environment\"\"\"\n",
    "    \n",
    "    def __init__(self, env = gym.make(\"LunarLander-v2\", render_mode = \"human\"), max_memory: int = 10_000):\n",
    "        \"\"\"\n",
    "        Create a new LLAgent. Initialize the q_network and the target_q_network\n",
    "        \n",
    "        Params:\n",
    "        env:\n",
    "            the environment that the agent will learn from\n",
    "        max_memory: int\n",
    "            the size of the memory space for the learning algorithm\n",
    "        \"\"\"\n",
    "        \n",
    "        self.env = env\n",
    "        self.q_function_input_size = env.observation_space.shape[0] + 1\n",
    "        self.q_function_output_size = 1\n",
    "        \n",
    "        self.q_function = self.get_q_function(self.q_function_input_size,\n",
    "                                              self.q_function_output_size)\n",
    "        self.q_target_function = self.get_q_function(self.q_function_input_size,\n",
    "                                              self.q_function_output_size)\n",
    "        self.replay_memory = deque(maxlen = max_memory) # Memory of items stored in the format [state, action, reward, next state, terminated]\n",
    "        \n",
    "    def get_optimum_action(self, state: np.ndarray, q_function: keras.Model) -> int:\n",
    "        \"\"\"\n",
    "        Return an optimal action based on the state\n",
    "        \n",
    "        Params:\n",
    "        state: np.ndarray\n",
    "            the numpy array representing the state for wchich an optimal action is needed\n",
    "        q_function: keras.Model\n",
    "            the model that will be used to predict the reward of each action\n",
    "        \n",
    "        Return:\n",
    "        int:\n",
    "            the int that is the optimal action to take\n",
    "        \"\"\"\n",
    "        states_actions = np.tile(np.append(state, [0]), (4, 1))\n",
    "        states_actions[:, -1] = np.array([0, 1, 2, 3]) * 0.25\n",
    "        return np.argmax(q_function.predict(states_actions))\n",
    "        \n",
    "    \n",
    "    def train_model(self, training_batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Based on new history data, train the model\n",
    "        \n",
    "        Params:\n",
    "        training_batch_size: int\n",
    "            the number of items to train over\n",
    "        \"\"\"\n",
    "        \n",
    "        indices = np.random.choice(len(self.replay_memory), training_batch_size)\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for index in indices:\n",
    "            # xs.append(self.replay_memory[index][0].append(self.replay_memory[index][1]))\n",
    "            xs.append(np.append(self.replay_memory[index][0][0], np.array([self.replay_memory[index][1]])))\n",
    "            if self.replay_memory[index][4] == True:\n",
    "                r = 0\n",
    "            else:\n",
    "                r = self.replay_memory[index][2]\n",
    "            \n",
    "            predicted_reward = self.q_target_function.predict(np.array([np.append(self.replay_memory[index][0][0], np.array([self.replay_memory[index][1]]))]), verbose = 0)[0] + r\n",
    "            ys.append(np.array([predicted_reward]))\n",
    "\n",
    "        self.q_function.fit(np.array(xs), np.array(ys))\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def train(self, trajectories: int, max_timesteps: int = None, steps_random: int = 20, epsilon = 0.4, training_period: int = 4, training_batch_size: int = 32, update_target_period: int = 200):\n",
    "        \"\"\"\n",
    "        Run the model for a specified number of trajectories, training the model as it goes.\n",
    "        \n",
    "        Params:\n",
    "        trajectories: int\n",
    "            the number of trajectories to run\n",
    "        max_timesteps: int\n",
    "            the max number of steps any single trajectory should take. If set to None, the trajectory will go till it has reached a terminal state\n",
    "        steps_random: int\n",
    "            the number of steps for which the agent should act completly randomly at the start of each episode\n",
    "        epsilon: int\n",
    "            the probability of making a random \n",
    "        training_period: int\n",
    "            the number of steps between each training\n",
    "        training_batch_size: int\n",
    "            the number of samples that will be trained over each training session\n",
    "        \"\"\"\n",
    "        \n",
    "        current_total_step = 0\n",
    "        \n",
    "        for _ in range(trajectories):\n",
    "            state = self.env.reset()\n",
    "            generate_trajectory = True\n",
    "            current_step = 0\n",
    "            \n",
    "            while generate_trajectory:\n",
    "                # Generate action\n",
    "                if current_step < steps_random:\n",
    "                    action = self.random_action()\n",
    "                else:\n",
    "                    make_random = self.epsilon_greedy_policy(epsilon)\n",
    "                    if make_random:\n",
    "                        action = self.random_action()\n",
    "                    else:\n",
    "                        action = self.get_optimum_action(state = state, q_function = self.q_function)\n",
    "                \n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                self.replay_memory.append([np.array(state), action, reward, next_state, terminated])\n",
    "                self.env.render()\n",
    "                \n",
    "                if current_step % training_period == 0 and len(self.replay_memory) > training_batch_size:\n",
    "                    # Train\n",
    "                    self.train_model(training_batch_size = training_batch_size)\n",
    "                state = next_state\n",
    "                current_step += 1\n",
    "                print(current_step)\n",
    "                \n",
    "                generate_trajectory = (max_timesteps is None or current_step < max_timesteps) and  not terminated\n",
    "\n",
    "                \n",
    "                if current_total_step % update_target_period == 0:\n",
    "                    self.q_target_function = keras.models.clone_model(self.q_function)\n",
    "                \n",
    "    \n",
    "    # HELPER METHODS\n",
    "    def get_q_function(self, \n",
    "                         input_size: int, \n",
    "                         output_size: int,\n",
    "                         num_layers: int = 3,\n",
    "                         layer_sizes: list[int] = [64, 32, 16],\n",
    "                         activation: str = \"relu\") -> keras.Model:\n",
    "        \"\"\"\n",
    "        Create a neural net to represent the q-function\n",
    "        \n",
    "        Params:\n",
    "        input_size: int\n",
    "            the size/dimensions of the function input (should be the shape of the observation space)\n",
    "        output_size: int\n",
    "            the size/dimensions of the function output (should be the shape of the action space)\n",
    "        num_layers: int\n",
    "            the number of hidden layers in the neural network\n",
    "        layer_sizes: list[int]\n",
    "            the sizes of each hidden layer: [hidden layer 1 size, hiddden layer 2 size...hidden layer -num-layyers- size]\n",
    "        activation: str\n",
    "            the activation function of the neural network\n",
    "        \"\"\"\n",
    "        # Assertions\n",
    "        assert num_layers == len(layer_sizes), f\"Number of layers must be the same as the length of layer sizes: num: {num_layers} != sizes: {len(layer_sizes)}\"\n",
    "        \n",
    "        # Build Neural Net\n",
    "        inputs = layers.Input(shape=(input_size,)) \n",
    "        layer = inputs\n",
    "        for layer_num in range(len(layer_sizes)):\n",
    "            layer = layers.Dense(layer_sizes[layer_num], activation = \"relu\")(layer)\n",
    "        output = layers.Dense(output_size, activation = \"sigmoid\")(layer)\n",
    "        \n",
    "        model = keras.Model(inputs = inputs, outputs = output)\n",
    "        model.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n",
    "        # model.summary()\n",
    "        return model\n",
    "    \n",
    "    def random_action(self) -> int:\n",
    "        \"\"\"Return a random number in the range [0, 3], representing a random action\"\"\"\n",
    "        return random.randint(0, 3)\n",
    "    \n",
    "    def epsilon_greedy_policy(self, epsilon: float) -> bool:\n",
    "        \"\"\"\n",
    "        Return true epsilon% of the time, otherwise return false\n",
    "        epsilon: float\n",
    "            the percent of time that the function should return random\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff18c3d2-95c5-4392-b84e-dfef80321209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purs0007\\AppData\\Local\\Temp\\ipykernel_2272\\2222254789.py:110: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.replay_memory.append([np.array(state), action, reward, next_state, terminated])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "21\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "22\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "23\n",
      "24\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "25\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "26\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "27\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "28\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "29\n",
      "30\n",
      "31\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "32\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\purs0007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\purs0007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\purs0007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\purs0007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\purs0007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\purs0007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_6\" is incompatible with the layer: expected shape=(None, 9), found shape=(None, 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m LLAgent()\n\u001b[1;32m----> 2\u001b[0m a\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn [9], line 115\u001b[0m, in \u001b[0;36mLLAgent.train\u001b[1;34m(self, trajectories, max_timesteps, steps_random, epsilon, training_period, training_batch_size, update_target_period)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_step \u001b[38;5;241m%\u001b[39m training_period \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_memory) \u001b[38;5;241m>\u001b[39m training_batch_size:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    117\u001b[0m current_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn [9], line 64\u001b[0m, in \u001b[0;36mLLAgent.train_model\u001b[1;34m(self, training_batch_size)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_memory[index][\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m---> 64\u001b[0m     predicted_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_target_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_memory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_memory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m r\n\u001b[0;32m     65\u001b[0m     ys\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray([predicted_reward]))\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_function\u001b[38;5;241m.\u001b[39mfit(np\u001b[38;5;241m.\u001b[39marray(xs), np\u001b[38;5;241m.\u001b[39marray(ys))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filegfoekn8r.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\purs0007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\purs0007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\purs0007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\purs0007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\purs0007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\purs0007\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_6\" is incompatible with the layer: expected shape=(None, 9), found shape=(None, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = LLAgent()\n",
    "a.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d3818a-57b7-4164-a37b-d3c169a5efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_trajectory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
