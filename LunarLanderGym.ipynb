{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5781e42e-6db3-496d-8746-13dc9b71c455",
   "metadata": {},
   "source": [
    "### Lunar Lander Environment\n",
    "Observation Space: 8-dimensional vector: the coordinates of the lander, its linear velocities, its angle, angular velocity, and bools representing whether each leg is touching the ground  \n",
    "[x, y, vx, vy, angle, angle_vel, left_leg_on_ground, right_leg_on_ground]  \n",
    "Action Space: 4 Discrete actions: do nothin, fire left orinetation engine, fire main engine, fire right orientation engine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582666d-bf8d-4c9f-9b0c-89e9546ae704",
   "metadata": {},
   "source": [
    "### Theory\n",
    "V(s) = max(Q(s,a)  \n",
    "Q(s,a) = R(s,a) + gV(s')  \n",
    "p(s) = max Q(s, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6593d1b2-b2b7-4c93-b5e5-72b45dc88dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0764c-ade3-4bf1-aa0d-49798c33f26c",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba427c7-fb73-4677-a47b-3a4a4bc7abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sys\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c77a01fe-c282-4eb3-a793-9f7761ac84c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLAgent:\n",
    "    \"\"\"A learning Agent for the Lunar Landar environment\"\"\"\n",
    "    \n",
    "    def __init__(self, env = gym.make(\"LunarLander-v2\", render_mode = \"human\"), max_memory: int = 10_000):\n",
    "        \"\"\"\n",
    "        Create a new LLAgent. Initialize the q_network and the target_q_network\n",
    "        \n",
    "        Params:\n",
    "        env:\n",
    "            the environment that the agent will learn from\n",
    "        max_memory: int\n",
    "            the size of the memory space for the learning algorithm\n",
    "        \"\"\"\n",
    "        \n",
    "        self.env = env\n",
    "        self.q_function_input_size = env.observation_space.shape[0] + 1\n",
    "        self.q_function_output_size = 1\n",
    "        \n",
    "        self.q_function = self.get_q_function(self.q_function_input_size,\n",
    "                                              self.q_function_output_size)\n",
    "        self.q_target_function = self.get_q_function(self.q_function_input_size,\n",
    "                                              self.q_function_output_size)\n",
    "        self.replay_memory = deque(maxlen = max_memory) # Memory of items stored in the format [state, action, reward, next state, terminated]\n",
    "        \n",
    "    def get_optimum_action(self, state: np.ndarray, q_function: keras.Model) -> int:\n",
    "        \"\"\"\n",
    "        Return an optimal action based on the state\n",
    "        \n",
    "        Params:\n",
    "        state: np.ndarray\n",
    "            the numpy array representing the state for wchich an optimal action is needed\n",
    "        q_function: keras.Model\n",
    "            the model that will be used to predict the reward of each action\n",
    "        \n",
    "        Return:\n",
    "        int:\n",
    "            the int that is the optimal action to take\n",
    "        \"\"\"\n",
    "        states_actions = np.tile(np.append(state, [0]), (4, 1))\n",
    "        states_actions[:, -1] = np.array([0, 1, 2, 3]) * 0.25\n",
    "        return np.argmax(q_function.predict(states_actions, verbose = 0))\n",
    "        \n",
    "    \n",
    "    def train_model(self, training_batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Based on new history data, train the model\n",
    "        \n",
    "        Params:\n",
    "        training_batch_size: int\n",
    "            the number of items to train over\n",
    "        \"\"\"\n",
    "        \n",
    "        indices = np.random.choice(len(self.replay_memory), training_batch_size)\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for i, index in enumerate(indices):\n",
    "            # xs.append(self.replay_memory[index][0].append(self.replay_memory[index][1]))\n",
    "            xs.append(np.append(self.replay_memory[index][0], np.array([self.replay_memory[index][1]])))\n",
    "            if self.replay_memory[index][4] == True:\n",
    "                r = 0\n",
    "            else:\n",
    "                r = self.replay_memory[index][2]\n",
    "            predicted_reward = self.q_target_function.predict(np.array(xs[i]).reshape((1, 9)), verbose = 0)[0] + r\n",
    "            ys.append(np.array([predicted_reward]))\n",
    "\n",
    "        self.q_function.fit(np.array(xs), np.array(ys))\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def train(self, trajectories: int, max_timesteps: int = None, steps_random: int = 20, epsilon = 0.4, training_period: int = 4, training_batch_size: int = 32, update_target_period: int = 200):\n",
    "        \"\"\"\n",
    "        Run the model for a specified number of trajectories, training the model as it goes.\n",
    "        \n",
    "        Params:\n",
    "        trajectories: int\n",
    "            the number of trajectories to run\n",
    "        max_timesteps: int\n",
    "            the max number of steps any single trajectory should take. If set to None, the trajectory will go till it has reached a terminal state\n",
    "        steps_random: int\n",
    "            the number of steps for which the agent should act completly randomly at the start of each episode\n",
    "        epsilon: int\n",
    "            the probability of making a random \n",
    "        training_period: int\n",
    "            the number of steps between each training\n",
    "        training_batch_size: int\n",
    "            the number of samples that will be trained over each training session\n",
    "        \"\"\"\n",
    "        \n",
    "        current_total_step = 0\n",
    "        \n",
    "        for _ in range(trajectories):\n",
    "            state = self.env.reset()[0]\n",
    "            generate_trajectory = True\n",
    "            current_step = 0\n",
    "            \n",
    "            while generate_trajectory:\n",
    "                # Generate action\n",
    "                if current_step < steps_random:\n",
    "                    action = self.random_action()\n",
    "                else:\n",
    "                    make_random = self.epsilon_greedy_policy(epsilon)\n",
    "                    if make_random:\n",
    "                        action = self.random_action()\n",
    "                    else:\n",
    "                        action = self.get_optimum_action(state = state, q_function = self.q_function)\n",
    "                \n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                self.replay_memory.append([np.array(state), action, reward, next_state, terminated])\n",
    "                self.env.render()\n",
    "                \n",
    "                if current_step % training_period == 0 and len(self.replay_memory) > training_batch_size:\n",
    "                    # Train\n",
    "                    self.train_model(training_batch_size = training_batch_size)\n",
    "                state = next_state\n",
    "                current_step += 1\n",
    "                \n",
    "                generate_trajectory = (max_timesteps is None or current_step < max_timesteps) and  not terminated\n",
    "\n",
    "                \n",
    "                if current_total_step % update_target_period == 0:\n",
    "                    self.q_target_function = keras.models.clone_model(self.q_function)\n",
    "                \n",
    "    \n",
    "    # HELPER METHODS\n",
    "    def get_q_function(self, \n",
    "                         input_size: int, \n",
    "                         output_size: int,\n",
    "                         num_layers: int = 3,\n",
    "                         layer_sizes: list[int] = [64, 32, 16],\n",
    "                         activation: str = \"relu\") -> keras.Model:\n",
    "        \"\"\"\n",
    "        Create a neural net to represent the q-function\n",
    "        \n",
    "        Params:\n",
    "        input_size: int\n",
    "            the size/dimensions of the function input (should be the shape of the observation space)\n",
    "        output_size: int\n",
    "            the size/dimensions of the function output (should be the shape of the action space)\n",
    "        num_layers: int\n",
    "            the number of hidden layers in the neural network\n",
    "        layer_sizes: list[int]\n",
    "            the sizes of each hidden layer: [hidden layer 1 size, hiddden layer 2 size...hidden layer -num-layyers- size]\n",
    "        activation: str\n",
    "            the activation function of the neural network\n",
    "        \"\"\"\n",
    "        # Assertions\n",
    "        assert num_layers == len(layer_sizes), f\"Number of layers must be the same as the length of layer sizes: num: {num_layers} != sizes: {len(layer_sizes)}\"\n",
    "        \n",
    "        # Build Neural Net\n",
    "        inputs = layers.Input(shape=(input_size,)) \n",
    "        layer = inputs\n",
    "        for layer_num in range(len(layer_sizes)):\n",
    "            layer = layers.Dense(layer_sizes[layer_num], activation = \"relu\")(layer)\n",
    "        output = layers.Dense(output_size, activation = \"sigmoid\")(layer)\n",
    "        \n",
    "        model = keras.Model(inputs = inputs, outputs = output)\n",
    "        model.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n",
    "        # model.summary()\n",
    "        return model\n",
    "    \n",
    "    def random_action(self) -> int:\n",
    "        \"\"\"Return a random number in the range [0, 3], representing a random action\"\"\"\n",
    "        return random.randint(0, 3)\n",
    "    \n",
    "    def epsilon_greedy_policy(self, epsilon: float) -> bool:\n",
    "        \"\"\"\n",
    "        Return true epsilon% of the time, otherwise return false\n",
    "        epsilon: float\n",
    "            the percent of time that the function should return random\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff18c3d2-95c5-4392-b84e-dfef80321209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 199ms/step - loss: 1.7872\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7787\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7623\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2849\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1262\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5045\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5178\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3915\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2137\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4511\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.7092\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7977\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8934\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3151\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0968\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7513\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.4215\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4896\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.4844\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.8099\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.6706\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.7945\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0268\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.3249\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3576\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2122\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8824\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2963\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3877\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2321\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3275\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4983\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.0650\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1721\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9303\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0174\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6956\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.0365\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4926\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7697\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5946\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9223\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.2692\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.6905\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8039\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5328\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3687\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.3827\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.3515\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5462\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0043\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4629\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0365\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.7437\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9706\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.8351\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.2472\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8964\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.5871\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.2266\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.7223\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.8760\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9282\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9181\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.5900\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9986\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1994\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6682\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.2572\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.8706\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9674\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.9983\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 3.7828\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.0481\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.8854\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0862\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.0185\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.4022\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7356\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 15.9734\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0147\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9171\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3694\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.0739\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 35.8358\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 144.3078\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.7499\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1919\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.3826\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 31.0697\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.0978\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 33.5761\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.3058\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 144.8079\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.4826\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.9828\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9126\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5801\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.2793\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2902\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.1268\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.1256\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13.4758\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.9581\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.0107\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.9689\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 6.7595\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.0672\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.5335\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2246\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4985\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.5865\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 37.8607\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.5968\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.6631\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.0416\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2134\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10.4331\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13.9299\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.0907\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.8089\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.8746\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 15.6248\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 20.3947\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.2999\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.4402\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.5723\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 22.7832\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 16.6761\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.9233\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13.7442\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.6793\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.9355\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 152.2861\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 37.3051\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.1134\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 16.1080\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.6164\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.5272\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13.0635\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.6113\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 15.5958\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.2253\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 15.4272\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20.4102\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10.4341\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.9279\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 20.2980\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.5769\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 19.8593\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.9906\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 19.9125\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.5310\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 42.5349\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 34.9732\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 16.7412\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.8655\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 150.6134\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.8302\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.1104\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.9677\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.0076\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.5373\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.0568\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 18.8390\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.3577\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.9870\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 16.0926\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 45.0154\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.7219\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 22.3136\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 17.0978\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.0709\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.4644\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 17.4640\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 23.0596\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25.2052\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 15.1128\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.8057\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1740\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.5252\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.4941\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 150.1180\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.9720\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.0452\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0173\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.1349\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.4010\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 36.2573\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.6921\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.2561\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.1100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 160.9865\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10.4819\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.4411\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10.3668\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 164.6977\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.4343\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.1530\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.5457\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23.6377\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 39.1889\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.7910\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.2925\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21.3352\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 14.1385\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.0215\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.2952\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.3493\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 14.6549\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.7668\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13.7892\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.6291\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.1188\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.1914\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21.3256\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 14.4728\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.7006\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.6641\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 15.6019\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 157.3873\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.3718\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.7711\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.4829\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 22.2966\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 43.6601\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 169.8934\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 20.0477\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.1183\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 13.1494\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.5168\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 16.4271\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 16.9506\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2137\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 19.5519\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.3526\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 21.0183\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.7260\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11.9851\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 16.4850\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 11.8189\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 15.7618\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 150.7680\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 16.7870\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 43.3009\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.1152\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.0192\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.9368\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11.2328\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.6680\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.0432\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.3053\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11.0718\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.3313\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.6953\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.0276\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 27.1144\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.0615\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23.4412\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 47.0372\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 44.8272\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 17.6955\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.5017\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11.8406\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0184\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 12.4918\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11.0726\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.7541\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 25.6427\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 8.8247\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 43.4397\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 20.4424\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.9989\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 18.9209\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 17.4736\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 9.9238\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6006\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11.0762\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 17.6420\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 24.3248\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 21.7507\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 11.7105\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.1267\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = LLAgent()\n",
    "a.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d3818a-57b7-4164-a37b-d3c169a5efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_trajectory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
